{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uav_active_sensing.config import PROCESSED_DATA_DIR\n",
    "from uav_active_sensing.pytorch_dataloaders import TinyImageNetDataset\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_processed_dir = PROCESSED_DATA_DIR / \"cifar10\"\n",
    "\n",
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=True, download=False, transform=cifar10_transform\n",
    ")\n",
    "cifar10_test_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=False, download=False, transform=cifar10_transform\n",
    ")\n",
    "\n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=1, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TINY_IMAGENET_PROCESSED_DIR = PROCESSED_DATA_DIR / \"tiny_imagenet/tiny-imagenet-200\"\n",
    "\n",
    "tiny_imagenet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "tiny_imagenet_train_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"train\", transform=tiny_imagenet_transform\n",
    ")\n",
    "tiny_imagenet_val_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"val\", transform=tiny_imagenet_transform\n",
    ")\n",
    "\n",
    "tiny_imagenet_train_loader = DataLoader(tiny_imagenet_train_dataset, batch_size=1, shuffle=True)\n",
    "tiny_imagenet_val_loader = DataLoader(tiny_imagenet_val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels for CIFAR-10\n",
    "classes = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Function to visualize images from a batch\n",
    "\n",
    "\n",
    "def visualize_batch(images, labels, classes, num_samples=1):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        image = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.title(classes[labels[i].item()])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a sample batch from the DataLoader\n",
    "sample_batch = next(iter(cifar10_train_loader))\n",
    "images, labels = sample_batch\n",
    "\n",
    "# Visualize the first 10 images from the batch\n",
    "visualize_batch(images, labels, classes, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tensors(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Visualizes two PyTorch tensors as images side by side.\n",
    "    If tensors are 3D (e.g., RGB), they will be shown as color images.\n",
    "    If tensors are 2D, they will be shown as grayscale images.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor to display.\n",
    "        tensor2 (torch.Tensor): The second tensor to display.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are at least 2D\n",
    "    if tensor1.dim() < 2 or tensor2.dim() < 2:\n",
    "        raise ValueError(\"Tensors must be at least 2D.\")\n",
    "\n",
    "    # If tensors are 3D, check if they represent RGB images (channels last)\n",
    "    if tensor1.dim() == 3 and tensor1.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor1 = tensor1.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "    if tensor2.dim() == 3 and tensor2.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor2 = tensor2.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot both tensors as images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # First tensor\n",
    "    axes[0].imshow(tensor1.numpy())\n",
    "    axes[0].set_title(\"Tensor 1\")\n",
    "    axes[0].axis('off')  # Hide axis\n",
    "\n",
    "    # Second tensor\n",
    "    axes[1].imshow(tensor2.numpy())\n",
    "    axes[1].set_title(\"Tensor 2\")\n",
    "    axes[1].axis('off')  # Hide axis\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv:\n",
    "    def __init__(\n",
    "        self, image: torch.Tensor,\n",
    "        img_FoV_ratio: int,\n",
    "        min_altitude: int,\n",
    "        max_altitude: int,\n",
    "        device: str = \"cpu\"\n",
    "    ) -> None:\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.img = image.to(self.device)\n",
    "        self.img_height, self.img_width = image.shape[1:]\n",
    "\n",
    "        self.img_sensor_ratio = img_FoV_ratio\n",
    "        self.min_altitude = min_altitude\n",
    "        self.max_altitude = max_altitude\n",
    "\n",
    "        self.sensor_height = self.img_height // self.img_sensor_ratio\n",
    "        self.sensor_width = self.img_width // self.img_sensor_ratio\n",
    "\n",
    "        self._sensor_pos = [0, 0, self.min_altitude] # This could be initialized randomly with env interface\n",
    "        self.sensor_min_pos = [0, 0, self.min_altitude]\n",
    "\n",
    "        self.max_zoom_level = 1\n",
    "        self.min_zoom_level = max(\n",
    "            self.img_height // self.sensor_height,\n",
    "            self.img_width // self.sensor_width)\n",
    "\n",
    "        self.sampled_img = torch.full_like(self.img, float('nan'), device=self.device)\n",
    "        self.sample_img_altitude_mask = torch.full_like(self.img, fill_value=self.max_altitude, dtype=torch.int32)\n",
    "\n",
    "    @property\n",
    "    def fov_bbox(self):\n",
    "        zoomed_size_height = int(self.sensor_height * self.zoom_level)\n",
    "        zoomed_size_width = int(self.sensor_width * self.zoom_level)\n",
    "\n",
    "        top = max(0, self.sensor_pos[0])\n",
    "        left = max(0, self.sensor_pos[1])\n",
    "        bottom = min(self.img_height, self.sensor_pos[0] + zoomed_size_height)\n",
    "        right = min(self.img_width, self.sensor_pos[1] + zoomed_size_width)\n",
    "\n",
    "        return [top, bottom, left, right]\n",
    "\n",
    "    @property\n",
    "    def sensor_max_pos(self):\n",
    "        top, bottom, left, right = self.fov_bbox\n",
    "\n",
    "        fov_height = bottom - top\n",
    "        fov_width = right - left\n",
    "        sensor_max_height = self.img_height - fov_height\n",
    "        sensor_max_width = self.img_width - fov_width\n",
    "\n",
    "        return (sensor_max_height, sensor_max_width, self.max_altitude)\n",
    "\n",
    "    @property\n",
    "    def sensor_pos(self):\n",
    "        return tuple(self._sensor_pos)\n",
    "\n",
    "    @sensor_pos.setter\n",
    "    def sensor_pos(self, new_position):\n",
    "        x, y, z = new_position\n",
    "\n",
    "        # z update first, in order to constrain x and y\n",
    "        self._sensor_pos[2] = max(min(z, self.sensor_max_pos[2]), self.sensor_min_pos[2])\n",
    "\n",
    "        # x and y update based on the new max positions\n",
    "        self._sensor_pos[0] = max(min(x, self.sensor_max_pos[0]), self.sensor_min_pos[0])\n",
    "        self._sensor_pos[1] = max(min(y, self.sensor_max_pos[1]), self.sensor_min_pos[1])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def zoom_level(self):\n",
    "        m = (self.min_zoom_level - self.max_zoom_level) / (self.max_altitude - self.min_altitude)\n",
    "        b = self.max_zoom_level - m * self.min_altitude\n",
    "\n",
    "        return m * self._sensor_pos[2] + b\n",
    "\n",
    "\n",
    "\n",
    "    def _apply_blur(self, window: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies an averaging blur to the input window tensor while considering margin artifacts.\n",
    "\n",
    "        Args:\n",
    "            window (torch.Tensor): A tensor of shape (C, H, W), where C is the number of channels,\n",
    "                                H is the height, and W is the width of the image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A blurred tensor with the same shape as the input window.\n",
    "        \"\"\"\n",
    "        kernel_size = int(self.zoom_level)\n",
    "        kernel_size = kernel_size if kernel_size % 2 == 1 else kernel_size + 1\n",
    "        padding = kernel_size // 2\n",
    "        window_padded = F.pad(window, (padding, padding, padding, padding), mode='reflect') # Apply reflection padding to avoid margin artifacts \n",
    "        blurred = F.avg_pool2d(window_padded.unsqueeze(0), kernel_size=kernel_size, stride=1, padding=0).squeeze(0)\n",
    "\n",
    "        return blurred\n",
    "\n",
    "\n",
    "    def _apply_higher_zoom_filter(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        top, bottom, left, right = self.fov_bbox\n",
    "\n",
    "        previous_altitude_mask = self.sample_img_altitude_mask[:, top:bottom, left:right]\n",
    "        current_altitude_mask = torch.full_like(previous_altitude_mask, fill_value=self.sensor_pos[2])\n",
    "\n",
    "        # Altitude mask updating\n",
    "        altitudes_to_update = current_altitude_mask < previous_altitude_mask\n",
    "        self.sample_img_altitude_mask[:, top:bottom, left:right][altitudes_to_update] = current_altitude_mask[altitudes_to_update]\n",
    "\n",
    "        # Observation updating\n",
    "        prev_obs = self.sampled_img[:, top:bottom, left:right]\n",
    "        obs_to_update = current_altitude_mask > previous_altitude_mask\n",
    "        obs[obs_to_update] = prev_obs[obs_to_update]\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _observe(self):\n",
    "        top, bottom, left, right = self.fov_bbox\n",
    "        obs = self.img[:, top:bottom, left:right].clone()\n",
    "\n",
    "        if self.zoom_level > self.max_zoom_level:\n",
    "            obs = self._apply_blur(obs)\n",
    "            obs = obs.squeeze(0)\n",
    "\n",
    "        obs = self._apply_higher_zoom_filter(obs)\n",
    "        self.sampled_img[:, top:bottom, left:right] = obs\n",
    "\n",
    "    def move(self, dx: int, dy: int, dz: int):\n",
    "        self.sensor_pos = (\n",
    "            self.sensor_pos[0] + dy,\n",
    "            self.sensor_pos[1] + dx,\n",
    "            self.sensor_pos[2] + dz\n",
    "        )\n",
    "\n",
    "        self._observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(env: ImageEnv, steps: int):\n",
    "    for _ in range(steps):\n",
    "        dx = random.choice([-5, 0, 5])  # Move left, stay, or move right\n",
    "        dy = random.choice([-5, 0, 5])  # Move up, stay, or move down\n",
    "        dz = random.choice([-5, 0, 5])  # Zoom in, stay, or zoom out\n",
    "\n",
    "        env.move(dx, dy, dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (2, 2) at dimension 2 of input [3, 4, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m image, label \u001b[38;5;241m=\u001b[39m tiny_imagenet_train_dataset[\u001b[38;5;241m15000\u001b[39m]\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m ImageEnv(image, IMG_FOV_RATIO, MIN_ALTITUDE, MAX_ALTITUDE)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrandom_walk\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m visualize_tensors(env\u001b[38;5;241m.\u001b[39mimg, env\u001b[38;5;241m.\u001b[39msampled_img)\n",
      "Cell \u001b[0;32mIn[124], line 7\u001b[0m, in \u001b[0;36mrandom_walk\u001b[0;34m(env, steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m dy \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Move up, stay, or move down\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dz \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Zoom in, stay, or zoom out\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 140\u001b[0m, in \u001b[0;36mImageEnv.move\u001b[0;34m(self, dx, dy, dz)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, dx: \u001b[38;5;28mint\u001b[39m, dy: \u001b[38;5;28mint\u001b[39m, dz: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m dy,\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dx,\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m dz\n\u001b[1;32m    138\u001b[0m     )\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[123], line 127\u001b[0m, in \u001b[0;36mImageEnv._observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg[:, top:bottom, left:right]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzoom_level \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_zoom_level:\n\u001b[0;32m--> 127\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    130\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_higher_zoom_filter(obs)\n",
      "Cell \u001b[0;32mIn[123], line 98\u001b[0m, in \u001b[0;36mImageEnv._apply_blur\u001b[0;34m(self, window)\u001b[0m\n\u001b[1;32m     96\u001b[0m kernel_size \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;28;01mif\u001b[39;00m kernel_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m kernel_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     97\u001b[0m padding \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m---> 98\u001b[0m window_padded \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Apply reflection padding to avoid margin artifacts \u001b[39;00m\n\u001b[1;32m     99\u001b[0m blurred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(window_padded\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blurred\n",
      "File \u001b[0;32m~/.virtualenvs/uav_active_sensing/lib/python3.12/site-packages/torch/nn/functional.py:5096\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   5089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   5090\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   5091\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   5092\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   5093\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   5094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5095\u001b[0m             )\u001b[38;5;241m.\u001b[39m_replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[0;32m-> 5096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (2, 2) at dimension 2 of input [3, 4, 1]"
     ]
    }
   ],
   "source": [
    "MIN_ALTITUDE = 10\n",
    "MAX_ALTITUDE = 100\n",
    "IMG_FOV_RATIO = 64\n",
    "\n",
    "image, label = tiny_imagenet_train_dataset[15000]\n",
    "env = ImageEnv(image, IMG_FOV_RATIO, MIN_ALTITUDE, MAX_ALTITUDE)\n",
    "\n",
    "\n",
    "random_walk(env, 1500)\n",
    "visualize_tensors(env.img, env.sampled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement image as environment, following common interface (gymnasium problably)\n",
    "# TODO: Implement reward model (with pretrained MAE) inside the env mechanics\n",
    "\n",
    "# Everything nice and decoupled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_active_sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
