{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uav_active_sensing.config import PROCESSED_DATA_DIR\n",
    "from uav_active_sensing.pytorch_dataloaders import TinyImageNetDataset\n",
    "from uav_active_sensing.env import ImageEnv\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_processed_dir = PROCESSED_DATA_DIR / \"cifar10\"\n",
    "\n",
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=True, download=False, transform=cifar10_transform\n",
    ")\n",
    "cifar10_test_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=False, download=False, transform=cifar10_transform\n",
    ")\n",
    "\n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=1, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TINY_IMAGENET_PROCESSED_DIR = PROCESSED_DATA_DIR / \"tiny_imagenet/tiny-imagenet-200\"\n",
    "\n",
    "tiny_imagenet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "tiny_imagenet_train_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"train\", transform=tiny_imagenet_transform\n",
    ")\n",
    "tiny_imagenet_val_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"val\", transform=tiny_imagenet_transform\n",
    ")\n",
    "\n",
    "tiny_imagenet_train_loader = DataLoader(tiny_imagenet_train_dataset, batch_size=1, shuffle=True)\n",
    "tiny_imagenet_val_loader = DataLoader(tiny_imagenet_val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels for CIFAR-10\n",
    "classes = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Function to visualize images from a batch\n",
    "\n",
    "\n",
    "def visualize_batch(images, labels, classes, num_samples=1):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        image = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.title(classes[labels[i].item()])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a sample batch from the DataLoader\n",
    "sample_batch = next(iter(cifar10_train_loader))\n",
    "images, labels = sample_batch\n",
    "\n",
    "# Visualize the first 10 images from the batch\n",
    "visualize_batch(images, labels, classes, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tensors(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Visualizes two PyTorch tensors as images side by side.\n",
    "    If tensors are 3D (e.g., RGB), they will be shown as color images.\n",
    "    If tensors are 2D, they will be shown as grayscale images.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor to display.\n",
    "        tensor2 (torch.Tensor): The second tensor to display.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are at least 2D\n",
    "    if tensor1.dim() < 2 or tensor2.dim() < 2:\n",
    "        raise ValueError(\"Tensors must be at least 2D.\")\n",
    "\n",
    "    # If tensors are 3D, check if they represent RGB images (channels last)\n",
    "    if tensor1.dim() == 3 and tensor1.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor1 = tensor1.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "    if tensor2.dim() == 3 and tensor2.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor2 = tensor2.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot both tensors as images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # First tensor\n",
    "    axes[0].imshow(tensor1.numpy())\n",
    "    axes[0].set_title(\"Tensor 1\")\n",
    "    axes[0].axis('off')  # Hide axis\n",
    "\n",
    "    # Second tensor\n",
    "    axes[1].imshow(tensor2.numpy())\n",
    "    axes[1].set_title(\"Tensor 2\")\n",
    "    axes[1].axis('off')  # Hide axis\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(env: ImageEnv, steps: int):\n",
    "    for _ in range(steps):\n",
    "        dx = random.choice([-5, 0, 5])  # Move left, stay, or move right\n",
    "        dy = random.choice([-5, 0, 5])  # Move up, stay, or move down\n",
    "        dz = random.choice([-5, 0, 5])  # Zoom in, stay, or zoom out\n",
    "\n",
    "        env.move(dx, dy, dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (4, 4) at dimension 2 of input [3, 8, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m image, label \u001b[38;5;241m=\u001b[39m tiny_imagenet_train_dataset[\u001b[38;5;241m15000\u001b[39m]\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m ImageEnv(image, IMG_FOV_RATIO, MIN_ALTITUDE, MAX_ALTITUDE)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrandom_walk\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m visualize_tensors(env\u001b[38;5;241m.\u001b[39mimg, env\u001b[38;5;241m.\u001b[39msampled_img)\n",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m, in \u001b[0;36mrandom_walk\u001b[0;34m(env, steps)\u001b[0m\n\u001b[1;32m      4\u001b[0m dy \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Move up, stay, or move down\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dz \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Zoom in, stay, or zoom out\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uav_active_sensing/uav_active_sensing/env.py:204\u001b[0m, in \u001b[0;36mImageEnv.move\u001b[0;34m(self, dx, dy, dz)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03mMoves the sensor by the specified amount along each axis and updates the observation.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    dz (int): The amount to move along the z-axis (altitude).\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m dy,\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dx,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m dz\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uav_active_sensing/uav_active_sensing/env.py:184\u001b[0m, in \u001b[0;36mImageEnv._observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg[:, top:bottom, left:right]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzoom_level \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_zoom_level:\n\u001b[0;32m--> 184\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    187\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_higher_zoom_filter(obs)\n",
      "File \u001b[0;32m~/uav_active_sensing/uav_active_sensing/env.py:142\u001b[0m, in \u001b[0;36mImageEnv._apply_blur\u001b[0;34m(self, window)\u001b[0m\n\u001b[1;32m    140\u001b[0m kernel_size \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;28;01mif\u001b[39;00m kernel_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m kernel_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    141\u001b[0m padding \u001b[38;5;241m=\u001b[39m kernel_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 142\u001b[0m window_padded \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply reflection padding to avoid margin artifacts\u001b[39;00m\n\u001b[1;32m    143\u001b[0m blurred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mavg_pool2d(window_padded\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39mkernel_size, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blurred\n",
      "File \u001b[0;32m~/.virtualenvs/uav_active_sensing/lib/python3.12/site-packages/torch/nn/functional.py:5096\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   5089\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   5090\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   5091\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   5092\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   5093\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[1;32m   5094\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5095\u001b[0m             )\u001b[38;5;241m.\u001b[39m_replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[0;32m-> 5096\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (4, 4) at dimension 2 of input [3, 8, 4]"
     ]
    }
   ],
   "source": [
    "MIN_ALTITUDE = 10\n",
    "MAX_ALTITUDE = 100\n",
    "IMG_FOV_RATIO = 64\n",
    "\n",
    "image, label = tiny_imagenet_train_dataset[15000]\n",
    "env = ImageEnv(image, IMG_FOV_RATIO, MIN_ALTITUDE, MAX_ALTITUDE)\n",
    "\n",
    "\n",
    "random_walk(env, 10000)\n",
    "visualize_tensors(env.img, env.sampled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement image as environment, following common interface (gymnasium problably)\n",
    "# TODO: Implement reward model (with pretrained MAE) inside the env mechanics\n",
    "\n",
    "# Everything nice and decoupled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_active_sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
