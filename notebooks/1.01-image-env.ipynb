{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uav_active_sensing.config import PROCESSED_DATA_DIR\n",
    "from uav_active_sensing.pytorch_dataloaders import TinyImageNetDataset\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_processed_dir = PROCESSED_DATA_DIR / \"cifar10\"\n",
    "\n",
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "cifar10_train_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=True, download=False, transform=cifar10_transform\n",
    ")\n",
    "cifar10_test_dataset = datasets.CIFAR10(\n",
    "    root=cifar10_processed_dir, train=False, download=False, transform=cifar10_transform\n",
    ")\n",
    "\n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=1, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TINY_IMAGENET_PROCESSED_DIR = PROCESSED_DATA_DIR / \"tiny_imagenet/tiny-imagenet-200\"\n",
    "\n",
    "tiny_imagenet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "tiny_imagenet_train_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"train\", transform=tiny_imagenet_transform\n",
    ")\n",
    "tiny_imagenet_val_dataset = TinyImageNetDataset(\n",
    "    root_dir=TINY_IMAGENET_PROCESSED_DIR, split=\"val\", transform=tiny_imagenet_transform\n",
    ")\n",
    "\n",
    "tiny_imagenet_train_loader = DataLoader(tiny_imagenet_train_dataset, batch_size=1, shuffle=True)\n",
    "tiny_imagenet_val_loader = DataLoader(tiny_imagenet_val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels for CIFAR-10\n",
    "classes = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Function to visualize images from a batch\n",
    "\n",
    "\n",
    "def visualize_batch(images, labels, classes, num_samples=1):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        image = images[i].permute(1, 2, 0).numpy()\n",
    "        plt.imshow(image)\n",
    "        plt.title(classes[labels[i].item()])\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a sample batch from the DataLoader\n",
    "sample_batch = next(iter(cifar10_train_loader))\n",
    "images, labels = sample_batch\n",
    "\n",
    "# Visualize the first 10 images from the batch\n",
    "visualize_batch(images, labels, classes, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tensors(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Visualizes two PyTorch tensors as images side by side.\n",
    "    If tensors are 3D (e.g., RGB), they will be shown as color images.\n",
    "    If tensors are 2D, they will be shown as grayscale images.\n",
    "\n",
    "    Args:\n",
    "        tensor1 (torch.Tensor): The first tensor to display.\n",
    "        tensor2 (torch.Tensor): The second tensor to display.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are at least 2D\n",
    "    if tensor1.dim() < 2 or tensor2.dim() < 2:\n",
    "        raise ValueError(\"Tensors must be at least 2D.\")\n",
    "\n",
    "    # If tensors are 3D, check if they represent RGB images (channels last)\n",
    "    if tensor1.dim() == 3 and tensor1.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor1 = tensor1.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "    if tensor2.dim() == 3 and tensor2.shape[0] in {3, 1}:  # RGB or grayscale\n",
    "        tensor2 = tensor2.permute(1, 2, 0)  # Convert (C, H, W) to (H, W, C)\n",
    "\n",
    "    # Plot both tensors as images\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # First tensor\n",
    "    axes[0].imshow(tensor1.numpy())\n",
    "    axes[0].set_title(\"Tensor 1\")\n",
    "    axes[0].axis('off')  # Hide axis\n",
    "\n",
    "    # Second tensor\n",
    "    axes[1].imshow(tensor2.numpy())\n",
    "    axes[1].set_title(\"Tensor 2\")\n",
    "    axes[1].axis('off')  # Hide axis\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv:\n",
    "    def __init__(\n",
    "        self, image: torch.Tensor,\n",
    "        img_FoV_ratio: int,\n",
    "        # min_altitude: int,\n",
    "        # max_altitude: int,\n",
    "        device: str = \"cpu\"\n",
    "    ) -> None:\n",
    "\n",
    "        if device is None:\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "        self.img = image.to(self.device)\n",
    "        self.img_height, self.img_width = image.shape[1:]\n",
    "\n",
    "        self.img_sensor_ratio = img_FoV_ratio\n",
    "\n",
    "        # self.min_altitude = min_altitude\n",
    "        # self.max_altitude = max_altitude\n",
    "        # self._altitude = self.min_altitude \n",
    "\n",
    "        self.sensor_height = self.img_height // self.img_sensor_ratio\n",
    "        self.sensor_width = self.img_width // self.img_sensor_ratio\n",
    "\n",
    "        self.sensor_min_pos = [0, 0]\n",
    "        self.sensor_max_pos = [self.img_height, self.img_width]\n",
    "        self._sensor_pos = [0, 0]  # This could be initialized randomly with env interface\n",
    "\n",
    "        self.min_kernel_size = 1\n",
    "        self.max_kernel_size = self.img_height - self.sensor_height + 1\n",
    "        self.max_kernel_size = self.max_kernel_size if self.max_kernel_size % 2 == 1 else self.max_kernel_size - 1\n",
    "        self._kernel_size = self.min_kernel_size\n",
    "\n",
    "        self.sampled_img = torch.full_like(self.img, float('nan'), device=self.device)\n",
    "        self.sampled_kernel_size_mask = torch.full_like(self.img, fill_value=self.max_kernel_size, dtype=torch.int32)\n",
    "\n",
    "    @property\n",
    "    def fov_bbox(self):\n",
    "        fov_height = int(self.sensor_height + self.kernel_size - 1)\n",
    "        fov_width = int(self.sensor_width + self.kernel_size - 1)\n",
    "\n",
    "        top = max(0, self.sensor_pos[0])\n",
    "        left = max(0, self.sensor_pos[1])\n",
    "        bottom = min(self.img_height, self.sensor_pos[0] + fov_height)\n",
    "        right = min(self.img_width, self.sensor_pos[1] + fov_width)\n",
    "\n",
    "        return [top, bottom, left, right]\n",
    "\n",
    "    # @property\n",
    "    # def sensor_max_pos(self):\n",
    "    #     top, bottom, left, right = self.fov_bbox\n",
    "\n",
    "    #     fov_height = bottom - top\n",
    "    #     fov_width = right - left\n",
    "    #     sensor_max_height = self.img_height - fov_height\n",
    "    #     sensor_max_width = self.img_width - fov_width\n",
    "\n",
    "    #     return (sensor_max_height, sensor_max_width)\n",
    "\n",
    "    @property\n",
    "    def sensor_pos(self):\n",
    "        \n",
    "        return tuple(self._sensor_pos)\n",
    "\n",
    "    @sensor_pos.setter\n",
    "    def sensor_pos(self, new_position):\n",
    "        x, y = new_position\n",
    "        self._sensor_pos[0] = max(min(x, self.sensor_max_pos[0]), self.sensor_min_pos[0])\n",
    "        self._sensor_pos[1] = max(min(y, self.sensor_max_pos[1]), self.sensor_min_pos[1])\n",
    "\n",
    "    @property\n",
    "    def kernel_size(self):\n",
    "\n",
    "        return self._kernel_size\n",
    "    \n",
    "    @kernel_size.setter\n",
    "    def kernel_size(self, kernel_size):\n",
    "        max_kernel_size = min(self.img_height, self.img_width) - min(self.sensor_pos)\n",
    "        max_kernel_size = max_kernel_size if max_kernel_size % 2 == 1 else max_kernel_size - 1\n",
    "\n",
    "        self._kernel_size = max(min(kernel_size, max_kernel_size), self.min_kernel_size)\n",
    "    \n",
    "    # @property\n",
    "    # def kernel_size(self):\n",
    "\n",
    "    #     return self._kernel_size\n",
    "    \n",
    "    # @kernel_size.setter\n",
    "    # def kernel_size(self, size: int):\n",
    "\n",
    "    #     self._kernel_size = max(min(size, self.max_available_kernel_size), self.min_kernel_size)\n",
    "\n",
    "    def _apply_blur(self, window: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies an averaging blur to the input window tensor while considering margin artifacts.\n",
    "\n",
    "        Args:\n",
    "            window (torch.Tensor): A tensor of shape (C, H, W), where C is the number of channels,\n",
    "                                H is the height, and W is the width of the image.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A blurred tensor with the same shape as the input window.\n",
    "        \"\"\"\n",
    "\n",
    "        padding = self.kernel_size // 2\n",
    "        window_padded = F.pad(window, (padding, padding, padding, padding), mode='reflect')  # Apply reflection padding to avoid margin artifacts\n",
    "        blurred = F.avg_pool2d(window_padded.unsqueeze(0), kernel_size=self.kernel_size, stride=1, padding=0).squeeze(0)\n",
    "\n",
    "        return blurred\n",
    "\n",
    "    def _apply_blur_filter(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        top, bottom, left, right = self.fov_bbox\n",
    "\n",
    "        prev_mask = self.sampled_kernel_size_mask[:, top:bottom, left:right]\n",
    "        curr_mask = torch.full_like(prev_mask, fill_value=self.max_kernel_size)\n",
    "\n",
    "        # Altitude mask updating\n",
    "        updated_mask = curr_mask < prev_mask\n",
    "        self.sampled_kernel_size_mask[:, top:bottom, left:right][updated_mask] = curr_mask[updated_mask]\n",
    "\n",
    "        # Observation updating\n",
    "        prev_obs = self.sampled_img[:, top:bottom, left:right]\n",
    "        obs_to_update = curr_mask > prev_mask\n",
    "        obs[obs_to_update] = prev_obs[obs_to_update]\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _observe(self):\n",
    "        top, bottom, left, right = self.fov_bbox\n",
    "        obs = self.img[:, top:bottom, left:right].clone()\n",
    "\n",
    "        if self.kernel_size > self.min_kernel_size:\n",
    "            obs = self._apply_blur(obs)\n",
    "            obs = obs.squeeze(0)\n",
    "\n",
    "        # obs = self._apply_blur_filter(obs)\n",
    "        self.sampled_img[:, top:bottom, left:right] = obs\n",
    "\n",
    "    def move(self, dx: int, dy: int, dz: int):\n",
    "        self.sensor_pos = (\n",
    "            self.sensor_pos[0] + 2 * dy,\n",
    "            self.sensor_pos[1] + dx,\n",
    "        )\n",
    "        self.kernel_size += dz\n",
    "\n",
    "        self._observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(env: ImageEnv, steps: int):\n",
    "    for _ in range(steps):\n",
    "        # try:\n",
    "        dx = random.choice([-5, 0, 5])  # Move left, stay, or move right\n",
    "        dy = random.choice([-5, 0, 5])  # Move up, stay, or move down\n",
    "        dz = random.choice([-1, 0, 1])  # Zoom in, stay, or zoom out\n",
    "\n",
    "        env.move(dx, dy, dz)\n",
    "        # except Exception as err:\n",
    "        #     print(err)\n",
    "        #     print(dx, dy, dz)\n",
    "        #     print(env.fov_bbox)\n",
    "        #     print(env.kernel_size)\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [3, 2, 2].  Tensor sizes: [3, 3, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m image, label \u001b[38;5;241m=\u001b[39m tiny_imagenet_train_dataset[\u001b[38;5;241m15080\u001b[39m]\n\u001b[1;32m      6\u001b[0m env \u001b[38;5;241m=\u001b[39m ImageEnv(image, IMG_FOV_RATIO)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrandom_walk\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m visualize_tensors(env\u001b[38;5;241m.\u001b[39mimg, env\u001b[38;5;241m.\u001b[39msampled_img)\n",
      "Cell \u001b[0;32mIn[174], line 8\u001b[0m, in \u001b[0;36mrandom_walk\u001b[0;34m(env, steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m dy \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m])  \u001b[38;5;66;03m# Move up, stay, or move down\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dz \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Zoom in, stay, or zoom out\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 147\u001b[0m, in \u001b[0;36mImageEnv.move\u001b[0;34m(self, dx, dy, dz)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m dy,\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msensor_pos[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m dx,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dz\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_observe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[173], line 138\u001b[0m, in \u001b[0;36mImageEnv._observe\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m     obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# obs = self._apply_blur_filter(obs)\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampled_img\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m:\u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m obs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (3) at non-singleton dimension 2.  Target sizes: [3, 2, 2].  Tensor sizes: [3, 3, 3]"
     ]
    }
   ],
   "source": [
    "MIN_ALTITUDE = 1\n",
    "MAX_ALTITUDE = 10\n",
    "IMG_FOV_RATIO = 64\n",
    "\n",
    "image, label = tiny_imagenet_train_dataset[15080]\n",
    "env = ImageEnv(image, IMG_FOV_RATIO)\n",
    "\n",
    "\n",
    "random_walk(env, 10_000)\n",
    "visualize_tensors(env.img, env.sampled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement image as environment, following common interface (gymnasium problably)\n",
    "# TODO: Implement reward model (with pretrained MAE) inside the env mechanics\n",
    "\n",
    "# Everything nice and decoupled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_active_sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
