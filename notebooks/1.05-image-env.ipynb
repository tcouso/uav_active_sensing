{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import gymnasium as gym\n",
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "\n",
    "from uav_active_sensing.pytorch_datasets import TinyImageNetDataset, tiny_imagenet_collate_fn\n",
    "from uav_active_sensing.modeling.img_env.img_exploration_env import ImageExplorationEnv, ImageExplorationEnvConfig, RewardFunction\n",
    "from uav_active_sensing.modeling.mae.act_vit_mae import ActViTMAEForPreTraining\n",
    "from uav_active_sensing.modeling.agents.rl_agent_feature_extractor import CustomResNetFeatureExtractor\n",
    "from uav_active_sensing.config import DEVICE\n",
    "from uav_active_sensing.plots import visualize_tensor, visualize_act_mae_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(1230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleImageDataset(Dataset):\n",
    "    def __init__(self, original_dataset: Dataset, index: int):\n",
    "        self.image, self.label = original_dataset[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image, self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/vit-mae-base\", use_fast=True)  # TODO: Download this in advance\n",
    "tiny_imagenet_train_dataset = TinyImageNetDataset(split=\"train\", transform=image_processor)\n",
    "random_index = rd.randint(0, len(tiny_imagenet_train_dataset) - 1)\n",
    "single_image_dataset = SingleImageDataset(tiny_imagenet_train_dataset, random_index)\n",
    "tiny_imagenet_train_loader = DataLoader(single_image_dataset, batch_size=2, collate_fn=tiny_imagenet_collate_fn)\n",
    "\n",
    "# Pretrained model and reward function\n",
    "act_mae_model = ActViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")  # TODO: Download this in advance\n",
    "reward_function = RewardFunction(act_mae_model,\n",
    "                                 num_samples=1,\n",
    "                                 reward_increase=False,\n",
    "                                 patch_size=16,\n",
    "                                 masking_ratio=0.8,\n",
    "                                 generator=generator,\n",
    "                                 )\n",
    "\n",
    "# Create a dummy environment to initialize the model\n",
    "img = next(iter(tiny_imagenet_train_loader))[0]  # Take one image as a dummy input for env initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = ImageExplorationEnvConfig(reward_function=reward_function, seed=45, sensor_size=16 * 2)\n",
    "env = ImageExplorationEnv(img, env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tcouso/.virtualenvs/uav_active_sensing/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:55: UserWarning: It seems that your observation sampled_img is an image but its `dtype` is (float32) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "/home/tcouso/.virtualenvs/uav_active_sensing/lib/python3.12/site-packages/stable_baselines3/common/env_checker.py:63: UserWarning: It seems that your observation space sampled_img is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()\n",
    "for j in range(128):\n",
    "    sample_action = env.action_space.sample()\n",
    "    # print(sample_action)\n",
    "    # print(env._denormalize_action(torch.from_numpy(sample_action)))\n",
    "    env.step(sample_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sampled_img = env._reward_function.sampled_img_random_masking(env.sampled_img)\n",
    "visualize_act_mae_reconstruction(env.img.unsqueeze(0), env.sampled_img.unsqueeze(0), masked_sampled_img.unsqueeze(0), act_mae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random masking of sampled image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = env.sampled_img.shape\n",
    "patch_size = 16\n",
    "masking_ratio = 0.5\n",
    "x = torch.clone(env.sampled_img)\n",
    "x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "num_patches_H = H // patch_size\n",
    "num_patches_W = W // patch_size\n",
    "\n",
    "kc, kh, kw = patch_size, patch_size, patch_size  # kernel size\n",
    "dc, dh, dw = patch_size, patch_size, patch_size  # stride\n",
    "\n",
    "patches = x.unfold(1, kc, dc).unfold(2, kh, dh)\n",
    "nan_mask = torch.isnan(patches)\n",
    "patch_nan_mask = nan_mask.any(dim=(3, 4, 5))\n",
    "valid_patches = ~patch_nan_mask\n",
    "valid_indices = torch.nonzero(valid_patches, as_tuple=True)\n",
    "\n",
    "num_valid = valid_indices[0].shape[0]  # Count of valid patches, error\n",
    "num_to_mask = int(masking_ratio * num_valid)  # Number of patches to mask\n",
    "\n",
    "mask_indices = torch.randperm(num_valid, generator=generator)[:num_to_mask]\n",
    "selected_patches = tuple(idx[mask_indices] for idx in valid_indices)  # Extract selected patch indices\n",
    "\n",
    "# Apply NaN masking\n",
    "patches[selected_patches] = float('nan')\n",
    "reconstructed = patches.permute(0, 3, 1, 4, 2, 5).view(B, C, num_patches_H * patch_size, num_patches_W * patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tensor(reconstructed)\n",
    "visualize_tensor(env.sampled_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampled_img_random_masking(sampled_img: torch.Tensor, masking_ratio: float, generator: torch.Generator) -> torch.Tensor:\n",
    "\n",
    "    B, C, H, W = sampled_img.shape\n",
    "    patch_size = 16\n",
    "    x = torch.clone(sampled_img)\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "    num_patches_H = H // patch_size\n",
    "    num_patches_W = W // patch_size\n",
    "\n",
    "    kc, kh = patch_size, patch_size  # kernel size\n",
    "    dc, dh = patch_size, patch_size  # stride\n",
    "\n",
    "    patches = x.unfold(1, kc, dc).unfold(2, kh, dh)\n",
    "    nan_mask = torch.isnan(patches)\n",
    "    patch_nan_mask = nan_mask.any(dim=(3, 4, 5))\n",
    "    valid_patches = ~patch_nan_mask\n",
    "    valid_indices = torch.nonzero(valid_patches, as_tuple=True)\n",
    "\n",
    "    num_valid = valid_indices[0].shape[0]  # Count of valid patches, error\n",
    "    num_to_mask = int(masking_ratio * num_valid)  # Number of patches to mask\n",
    "\n",
    "    mask_indices = torch.randperm(num_valid, generator=generator)[:num_to_mask]\n",
    "    selected_patches = tuple(idx[mask_indices] for idx in valid_indices)  # Extract selected patch indices\n",
    "\n",
    "    # Apply NaN masking\n",
    "    patches[selected_patches] = float('nan')\n",
    "\n",
    "    # Reassemble image from patches\n",
    "    reconstructed = patches.permute(0, 3, 1, 4, 2, 5).view(B, C, num_patches_H * patch_size, num_patches_W * patch_size)\n",
    "\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test kernel size increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = ImageExplorationEnvConfig(reward_function=reward_function, seed=0)\n",
    "env = ImageExplorationEnv(img, env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()\n",
    "\n",
    "for i in range(env.img_h // env.sensor_h // 2 - 1):\n",
    "    sample_action = np.array([[0, 0, 1] for i in range(env.batch_size)])\n",
    "    env.step(sample_action)\n",
    "    # env._sensor_pos = env.sensor_max_pos_from_kernel_size\n",
    "    # print(env._kernel_size)\n",
    "    # print(env.sensor_max_pos_from_kernel_size)\n",
    "    # print(env.fov_bbox)\n",
    "\n",
    "for k in range(env.batch_size):\n",
    "    visualize_tensor(env.img[k])\n",
    "    visualize_tensor(env.sampled_img[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test deterministic behaviour for a given seed-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = ImageExplorationEnvConfig(img=dummy_batch, reward_function=reward_function)\n",
    "env = ImageExplorationEnv(env_config)\n",
    "env.reset()\n",
    "first_run_actions = []\n",
    "for j in range(5):\n",
    "    sample_action = env.action_space.sample()\n",
    "    # print(sample_action)\n",
    "    # print(env._denormalize_action(torch.from_numpy(sample_action)))\n",
    "    env.step(sample_action)\n",
    "    first_run_actions.append(sample_action)\n",
    "\n",
    "first_run = env.sampled_img.detach().clone()\n",
    "\n",
    "env_config = ImageExplorationEnvConfig(img=dummy_batch, reward_function=reward_function)\n",
    "env = ImageExplorationEnv(env_config)\n",
    "second_run_actions = []\n",
    "env.reset()\n",
    "for j in range(5):\n",
    "    sample_action = env.action_space.sample()\n",
    "    # print(sample_action)\n",
    "    # print(env._denormalize_action(torch.from_numpy(sample_action)))\n",
    "    env.step(sample_action)\n",
    "    second_run_actions.append(sample_action)\n",
    "\n",
    "second_run = env.sampled_img.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images shoud be identical if the sampling is deterministic (fixed seed)\n",
    "\n",
    "for j in range(5):\n",
    "    print((first_run_actions[j] == second_run_actions[j]).all())\n",
    "\n",
    "for k in range(env.batch_size):\n",
    "    visualize_tensor(first_run[k])\n",
    "    visualize_tensor(second_run[k])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uav_active_sensing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
